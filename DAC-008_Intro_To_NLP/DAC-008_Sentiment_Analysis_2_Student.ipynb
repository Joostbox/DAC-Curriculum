{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dl2ErmA4DcoN"
   },
   "source": [
    "## Table of Contents:\n",
    "Sentiment Analysis of Movie Reviews:\n",
    "1. EDA\n",
    "2. Data Preprocessing\n",
    "3. Feature Extraction\n",
    "4. ML modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1fChb-R-Fu9I"
   },
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 58875,
     "status": "ok",
     "timestamp": 1699962423477,
     "user": {
      "displayName": "Parth Uday",
      "userId": "17936957073811628306"
     },
     "user_tz": -480
    },
    "id": "ioLRjA94FiyG",
    "outputId": "3eff86ef-24d4-460a-80cb-e4afc2544e98"
   },
   "outputs": [],
   "source": [
    "# Linear algebra\n",
    "import numpy as np\n",
    "# EDA\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# NLTK libraries\n",
    "import nltk\n",
    "#nltk.download('all')    # After running all, comment out this line to stop redownloading nltk every time\n",
    "# Stopwords\n",
    "from nltk.corpus import stopwords\n",
    "# Stemmer & Lemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer,WordNetLemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# Wordcloud\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "# Tokenizer\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "# RE\n",
    "import re,string,unicodedata\n",
    "# Bag of Words\n",
    "from textblob import TextBlob\n",
    "from textblob import Word\n",
    "# Feature Extraction\n",
    "from sklearn.model_selection import train_test_split, cross_val_score,StratifiedShuffleSplit\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "# ML models\n",
    "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "# Metrics\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Web Scraping tool\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lz7-hE3dQu4x"
   },
   "source": [
    "### Importing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "PeScqdmrPKpV"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('datasets/IMDB.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extend the dataframe display size\n",
    "pd.options.display.max_colwidth = 110"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lPZ-wdnUS4C8"
   },
   "source": [
    "### Inspecting dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 334,
     "status": "ok",
     "timestamp": 1699962432172,
     "user": {
      "displayName": "Parth Uday",
      "userId": "17936957073811628306"
     },
     "user_tz": -480
    },
    "id": "5lrwVLJCQ4Bc",
    "outputId": "0dd2f00f-b259-419c-b48f-db389ee5278f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The filming technique is very unassuming- very old-time-BBC fas...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air condition...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy (Jake) thinks there's a zombie in his closet &amp; his parents a...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is a visually stunning film to watch. Mr. Mattei offers us a v...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                          review  \\\n",
       "0  One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are ...   \n",
       "1  A wonderful little production. <br /><br />The filming technique is very unassuming- very old-time-BBC fas...   \n",
       "2  I thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air condition...   \n",
       "3  Basically there's a family where a little boy (Jake) thinks there's a zombie in his closet & his parents a...   \n",
       "4  Petter Mattei's \"Love in the Time of Money\" is a visually stunning film to watch. Mr. Mattei offers us a v...   \n",
       "\n",
       "  sentiment  \n",
       "0  positive  \n",
       "1  positive  \n",
       "2  positive  \n",
       "3  negative  \n",
       "4  positive  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1699962434737,
     "user": {
      "displayName": "Parth Uday",
      "userId": "17936957073811628306"
     },
     "user_tz": -480
    },
    "id": "xceQYBrzSLKS",
    "outputId": "18f1f0c1-be5b-4f47-914d-57395aa0a1d4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rs7l-jq4TKmE"
   },
   "source": [
    "## 1. EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "executionInfo": {
     "elapsed": 296,
     "status": "ok",
     "timestamp": 1699962438647,
     "user": {
      "displayName": "Parth Uday",
      "userId": "17936957073811628306"
     },
     "user_tz": -480
    },
    "id": "zRUYusYlTIYc",
    "outputId": "7a8a0c70-7a46-4968-df0f-f1cbcdc06e18"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50000</td>\n",
       "      <td>50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>49582</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Loved today's show!!! It was a variety and not solely cooking (which would have been great too). Very stim...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>5</td>\n",
       "      <td>25000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                               review  \\\n",
       "count                                                                                                           50000   \n",
       "unique                                                                                                          49582   \n",
       "top     Loved today's show!!! It was a variety and not solely cooking (which would have been great too). Very stim...   \n",
       "freq                                                                                                                5   \n",
       "\n",
       "       sentiment  \n",
       "count      50000  \n",
       "unique         2  \n",
       "top     positive  \n",
       "freq       25000  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summary of the dataset\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 371,
     "status": "ok",
     "timestamp": 1699962448490,
     "user": {
      "displayName": "Parth Uday",
      "userId": "17936957073811628306"
     },
     "user_tz": -480
    },
    "id": "HdEh23MkTQ_Z",
    "outputId": "2c289733-8d5c-44b6-c309-f6c9f8d6a8b7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "positive    25000\n",
       "negative    25000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Counting sentiments\n",
    "data['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "En8UoW5RUz1x"
   },
   "source": [
    "## 2. Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "executionInfo": {
     "elapsed": 315,
     "status": "ok",
     "timestamp": 1699962460023,
     "user": {
      "displayName": "Parth Uday",
      "userId": "17936957073811628306"
     },
     "user_tz": -480
    },
    "id": "ZzAPFuhF7ibM",
    "outputId": "e0c6ee9d-7257-4f10-e437-66855f8ad7dd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The filming technique is very unassuming- very old-time-BBC fas...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air condition...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy (Jake) thinks there's a zombie in his closet &amp; his parents a...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is a visually stunning film to watch. Mr. Mattei offers us a v...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>I thought this movie did a down right good job. It wasn't as creative or original as the first, but who wa...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>Bad plot, bad dialogue, bad acting, idiotic directing, the annoying porn groove soundtrack that ran contin...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I am a Catholic taught in parochial elementary schools by nuns, taught by Jesuit priests in high school &amp; ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>I'm going to have to disagree with the previous comment and side with Maltin on this one. This is a second...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>No one expects the Star Trek movies to be high art, but the fans do expect a movie that is as good as some...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                              review  \\\n",
       "0      One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are ...   \n",
       "1      A wonderful little production. <br /><br />The filming technique is very unassuming- very old-time-BBC fas...   \n",
       "2      I thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air condition...   \n",
       "3      Basically there's a family where a little boy (Jake) thinks there's a zombie in his closet & his parents a...   \n",
       "4      Petter Mattei's \"Love in the Time of Money\" is a visually stunning film to watch. Mr. Mattei offers us a v...   \n",
       "...                                                                                                              ...   \n",
       "49995  I thought this movie did a down right good job. It wasn't as creative or original as the first, but who wa...   \n",
       "49996  Bad plot, bad dialogue, bad acting, idiotic directing, the annoying porn groove soundtrack that ran contin...   \n",
       "49997  I am a Catholic taught in parochial elementary schools by nuns, taught by Jesuit priests in high school & ...   \n",
       "49998  I'm going to have to disagree with the previous comment and side with Maltin on this one. This is a second...   \n",
       "49999  No one expects the Star Trek movies to be high art, but the fans do expect a movie that is as good as some...   \n",
       "\n",
       "      sentiment  \n",
       "0      positive  \n",
       "1      positive  \n",
       "2      positive  \n",
       "3      negative  \n",
       "4      positive  \n",
       "...         ...  \n",
       "49995  positive  \n",
       "49996  negative  \n",
       "49997  negative  \n",
       "49998  negative  \n",
       "49999  negative  \n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quick visualisation of dataset (First 5 + Last 5 rows)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of the 'data' dataframe to work off from\n",
    "data_IMDB = data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "XY1V4mz_Tf1m"
   },
   "outputs": [],
   "source": [
    "# Initialize the tokenizer\n",
    "tokenizer = ToktokTokenizer()\n",
    "\n",
    "# Setting English stopwords\n",
    "stopword = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1699933674001,
     "user": {
      "displayName": "Parth Uday",
      "userId": "17936957073811628306"
     },
     "user_tz": -480
    },
    "id": "gGxBnkil6u4N",
    "outputId": "45f7b6e6-a83e-4637-d973-50f96d165da9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'an', 'example', 'sentence', 'for', 'tokenization', '.']\n"
     ]
    }
   ],
   "source": [
    "# Example text to tokenize\n",
    "text = \"This is an example sentence for tokenization.\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "# # Print the tokens\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0YcKKbTZXkdX"
   },
   "source": [
    "### Text Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "RZaeg2-nV7aJ"
   },
   "outputs": [],
   "source": [
    "# Removing the html strips\n",
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "# Removing the square brackets\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "\n",
    "# Removing special characters\n",
    "def remove_special_characters(text, remove_digits=True):\n",
    "    pattern = r'[^a-zA-z0-9\\s]'\n",
    "    text = re.sub(pattern,'',text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 307,
     "status": "ok",
     "timestamp": 1699934775369,
     "user": {
      "displayName": "Parth Uday",
      "userId": "17936957073811628306"
     },
     "user_tz": -480
    },
    "id": "Sptmkrsl7v9A",
    "outputId": "d865fb27-732e-4140-a8f0-c0090b64ecbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is bold and italic.\n",
      "This is  with  sets of .\n",
      "Hello world 123\n"
     ]
    }
   ],
   "source": [
    "# Example of our 3 defined functions\n",
    "html_text = \"<p>This is <b>bold</b> and <i>italic</i>.</p>\"\n",
    "plain_text = strip_html(html_text)\n",
    "print(plain_text)\n",
    "\n",
    "\n",
    "input_text = \"This is [some text] with [multiple] sets of [square brackets].\"\n",
    "result = remove_between_square_brackets(input_text)\n",
    "print(result)\n",
    "\n",
    "input_text = \"Hello, @world! 123\"\n",
    "result2 = remove_special_characters(input_text)\n",
    "print(result2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "NobuMs_1YHKo"
   },
   "outputs": [],
   "source": [
    "# Removing the noisy text\n",
    "def denoise_text(text):\n",
    "    text = strip_html(text)\n",
    "    text = remove_between_square_brackets(text)\n",
    "    text = remove_special_characters(text)\n",
    "    return text\n",
    "\n",
    "# Apply function on review column\n",
    "data_IMDB['review'] = data_IMDB['review'].apply(denoise_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        One of the other reviewers has mentioned that after watching just 1 Oz episode youll be hooked They are ri...\n",
       "1        A wonderful little production The filming technique is very unassuming very oldtimeBBC fashion and gives a...\n",
       "2        I thought this was a wonderful way to spend time on a too hot summer weekend sitting in the air conditione...\n",
       "3        Basically theres a family where a little boy Jake thinks theres a zombie in his closet  his parents are fi...\n",
       "4        Petter Matteis Love in the Time of Money is a visually stunning film to watch Mr Mattei offers us a vivid ...\n",
       "                                                             ...                                                      \n",
       "49995    I thought this movie did a down right good job It wasnt as creative or original as the first but who was e...\n",
       "49996    Bad plot bad dialogue bad acting idiotic directing the annoying porn groove soundtrack that ran continuall...\n",
       "49997    I am a Catholic taught in parochial elementary schools by nuns taught by Jesuit priests in high school  co...\n",
       "49998    Im going to have to disagree with the previous comment and side with Maltin on this one This is a second r...\n",
       "49999    No one expects the Star Trek movies to be high art but the fans do expect a movie that is as good as some ...\n",
       "Name: review, Length: 50000, dtype: object"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_IMDB['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "st6343ngY31g"
   },
   "outputs": [],
   "source": [
    "# Lemmatizing the text\n",
    "def simple_lemmatize(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = ' '.join({lemmatizer.lemmatize(word) for word in text.split()})\n",
    "    return text\n",
    "\n",
    "# Apply function on review column\n",
    "data_IMDB['review'] = data_IMDB['review'].apply(simple_lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 320,
     "status": "ok",
     "timestamp": 1699936065808,
     "user": {
      "displayName": "Parth Uday",
      "userId": "17936957073811628306"
     },
     "user_tz": -480
    },
    "id": "U1m6gf1RC1H8",
    "outputId": "9151db4b-177f-4d00-ca63-363ecc586de8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I and are am car fast. eating. running The\n"
     ]
    }
   ],
   "source": [
    "# Example of lemmatizing text using sample sentence\n",
    "input_text = \"I am running and eating. The cars are running fast.\"\n",
    "lemmatized_text = simple_lemmatize(input_text)\n",
    "print(lemmatized_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M2J-v8LfHwZp"
   },
   "source": [
    "### Initial Data-Preprocessing Verdict\n",
    "Lemmatizer is not as accurate as we want it to be\n",
    "- Sentences are garbled and in a mess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-Of-Speech (POS) tagging\n",
    "Implement Part-Of-Speech (POS) tagging to improve accuracy.\n",
    "\n",
    "This helps the algorithm understand the grammatical structure and meaning of a text.\n",
    "\n",
    "For example, consider the sentence: \"The cat is sleeping on the mat.\"\n",
    "\n",
    "POS tagging would assign the following tags:\n",
    "- \"The\" - determiner (DT)\n",
    "- \"cat\" - noun (NN)\n",
    "- \"is\" - verb (VBZ)\n",
    "- \"sleeping\" - verb (VBG)\n",
    "- \"on\" - preposition (IN)\n",
    "- \"the\" - determiner (DT)\n",
    "- \"mat\" - noun (NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 300,
     "status": "ok",
     "timestamp": 1699963521269,
     "user": {
      "displayName": "Parth Uday",
      "userId": "17936957073811628306"
     },
     "user_tz": -480
    },
    "id": "64tl9OmVGP7y",
    "outputId": "d51da1c5-ccb7-467f-f47b-28aeb6a6a8eb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/joostbox/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/joostbox/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/joostbox/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Function to process the pos_tag\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('N'):\n",
    "        return 'n'  # Noun\n",
    "    elif tag.startswith('V'):\n",
    "        return 'v'  # Verb\n",
    "    elif tag.startswith('R'):\n",
    "        return 'r'  # Adverb\n",
    "    elif tag.startswith('J'):\n",
    "        return 'a'  # Adjective\n",
    "    else:\n",
    "        return 'n'  # Default to noun for unknown or uncategorized words\n",
    "\n",
    "# Redefining the lemmatizer function\n",
    "def simple_lemmatize(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(text)\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word, pos=get_wordnet_pos(tag)) for word, tag in pos_tags]\n",
    "    lemmatized_text = ' '.join(lemmatized_tokens)\n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 508,
     "status": "ok",
     "timestamp": 1699963567194,
     "user": {
      "displayName": "Parth Uday",
      "userId": "17936957073811628306"
     },
     "user_tz": -480
    },
    "id": "jiiP5IcNHDkv",
    "outputId": "7dac2cc6-f252-4f66-e78b-1a4389f2a1e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I be run and eat . The car be run fast .\n"
     ]
    }
   ],
   "source": [
    "# Test if the new function works as intended\n",
    "input_text = \"I am running and eating. The cars are running fast.\"\n",
    "lemmatized_text = simple_lemmatize(input_text)\n",
    "print(lemmatized_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "9_35d4QVOOhG"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[99], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Can we apply the new function on review column?\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m data_IMDB[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreview\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdata_IMDB\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreview\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43msimple_lemmatize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# We are not going to do it this way; computationally expensive, time consuming\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/DAC Respository/DAC-Curriculum/.venv/lib/python3.9/site-packages/pandas/core/series.py:4917\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4800\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/DAC Respository/DAC-Curriculum/.venv/lib/python3.9/site-packages/pandas/core/apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/DAC Respository/DAC-Curriculum/.venv/lib/python3.9/site-packages/pandas/core/apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/Documents/DAC Respository/DAC-Curriculum/.venv/lib/python3.9/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/DAC Respository/DAC-Curriculum/.venv/lib/python3.9/site-packages/pandas/core/algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1747\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[97], line 24\u001b[0m, in \u001b[0;36msimple_lemmatize\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msimple_lemmatize\u001b[39m(text):\n\u001b[1;32m     23\u001b[0m     lemmatizer \u001b[38;5;241m=\u001b[39m WordNetLemmatizer()\n\u001b[0;32m---> 24\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     pos_tags \u001b[38;5;241m=\u001b[39m pos_tag(tokens)\n\u001b[1;32m     26\u001b[0m     lemmatized_tokens \u001b[38;5;241m=\u001b[39m [lemmatizer\u001b[38;5;241m.\u001b[39mlemmatize(word, pos\u001b[38;5;241m=\u001b[39mget_wordnet_pos(tag)) \u001b[38;5;28;01mfor\u001b[39;00m word, tag \u001b[38;5;129;01min\u001b[39;00m pos_tags]\n",
      "File \u001b[0;32m~/Documents/DAC Respository/DAC-Curriculum/.venv/lib/python3.9/site-packages/nltk/tokenize/__init__.py:143\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03mReturn a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03musing NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m:type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    142\u001b[0m sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    144\u001b[0m     token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    145\u001b[0m ]\n",
      "File \u001b[0;32m~/Documents/DAC Respository/DAC-Curriculum/.venv/lib/python3.9/site-packages/nltk/tokenize/__init__.py:144\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03mReturn a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03musing NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m:type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    142\u001b[0m sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m--> 144\u001b[0m     token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_treebank_word_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m ]\n",
      "File \u001b[0;32m~/Documents/DAC Respository/DAC-Curriculum/.venv/lib/python3.9/site-packages/nltk/tokenize/destructive.py:182\u001b[0m, in \u001b[0;36mNLTKWordTokenizer.tokenize\u001b[0;34m(self, text, convert_parentheses, return_str)\u001b[0m\n\u001b[1;32m    179\u001b[0m     text \u001b[38;5;241m=\u001b[39m regexp\u001b[38;5;241m.\u001b[39msub(substitution, text)\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m regexp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCONTRACTIONS2:\n\u001b[0;32m--> 182\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[43mregexp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43m1 \u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43m2 \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m regexp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCONTRACTIONS3:\n\u001b[1;32m    184\u001b[0m     text \u001b[38;5;241m=\u001b[39m regexp\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m1 \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m2 \u001b[39m\u001b[38;5;124m\"\u001b[39m, text)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Can we apply the new function on review column?\n",
    "data_IMDB['review'] = data_IMDB['review'].apply(simple_lemmatize)\n",
    "# We are not going to do it this way; computationally expensive, time consuming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 304,
     "status": "ok",
     "timestamp": 1699963826066,
     "user": {
      "displayName": "Parth Uday",
      "userId": "17936957073811628306"
     },
     "user_tz": -480
    },
    "id": "eadnMU3TZS73",
    "outputId": "759806e5-89a3-4458-dac9-ba97d4394533"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'they', 'such', 'against', 'or', 'again', 'that', 'was', 'into', 'the', 'than', 'we', 'while', 'more', 'm', \"you'll\", 'being', 'both', 'should', 'weren', 'themselves', 'you', 'ourselves', 'ours', 'on', \"weren't\", 'been', 'she', 'once', \"aren't\", 'isn', 'haven', \"should've\", 'why', 'myself', \"you're\", \"isn't\", 'between', 'just', 'yourself', 'down', 'by', \"you've\", 'because', 'mustn', 'these', 're', 'of', 'doing', 'herself', 'yours', 'off', 'can', \"haven't\", 'did', 'needn', \"hadn't\", \"mightn't\", 'this', 'nor', 'am', 'i', 'any', 'have', 'y', 'so', \"doesn't\", 'yourselves', 'which', 'it', 'what', 'our', 've', 'couldn', \"that'll\", 'below', 'all', \"shan't\", 'after', 'your', 'be', 'to', \"mustn't\", 'shan', 't', 'having', 'itself', 'their', 'no', 'until', 'now', \"you'd\", 'other', 'don', 'over', 'some', 'will', 'me', 'under', 'aren', 'then', 'as', 'from', \"wouldn't\", 'if', 'has', 'there', 'about', 'up', 'him', \"couldn't\", 'them', 'in', \"needn't\", 'few', 'wasn', 'not', 'do', 'further', 'same', 'd', 'mightn', 'had', 'shouldn', 'won', 'is', 'my', \"it's\", 'here', 'he', 'those', 'very', 'o', 'a', \"won't\", 'most', 'whom', 'are', 's', \"hasn't\", \"don't\", 'but', 'where', 'does', 'for', 'ain', 'how', 'before', 'hers', 'only', 'who', 'her', 'ma', 'through', 'll', \"shouldn't\", 'doesn', 'with', 'when', 'and', 'too', 'during', 'its', \"wasn't\", 'theirs', 'above', 'hasn', 'an', \"didn't\", 'own', 'his', 'himself', 'wouldn', 'hadn', \"she's\", 'at', 'out', 'didn', 'each', 'were'}\n",
      "This example sentence stopwords .\n"
     ]
    }
   ],
   "source": [
    "# Set stopwords to English\n",
    "stop = set(stopwords.words('english'))\n",
    "print(stop)\n",
    "\n",
    "# Removing the stopwords\n",
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopword]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopword]\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text\n",
    "\n",
    "# Example to test out our stopword-removing function\n",
    "input_text = \"This is an example sentence with some stopwords.\"\n",
    "filtered_text = remove_stopwords(input_text, is_lower_case=True)\n",
    "print(filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "4dhovTtzODXE"
   },
   "outputs": [],
   "source": [
    "# Apply function on 'review' column\n",
    "data_IMDB['review'] = data_IMDB['review'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5cli3CZ2cfnk"
   },
   "source": [
    "### Text Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "executionInfo": {
     "elapsed": 308,
     "status": "ok",
     "timestamp": 1699964050294,
     "user": {
      "displayName": "Parth Uday",
      "userId": "17936957073811628306"
     },
     "user_tz": -480
    },
    "id": "pU4Dl2a8cjI7",
    "outputId": "45f5eac5-31b6-49e7-864a-f35c04c90d95"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>right Security hardcore unflinching GO guard City Christians glass watched may called timid wordIt thing b...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>give sense guard discomforting really guided Orton master surface thing dream extremely got unassuming lif...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>likable Match right Risk theater hot Addiction Prada Woody interesting Wears womanThis air spend may disap...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>film make Decide thriller BOOGEYMAN soap youre timeThis 10 boy zombie zombieOK watched ruin real life shot...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>loneliness best film make place look Imperioli Carol meet watch Money find Arthur connected aliveWe thing ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>right job always pay DVD watch touching original people count wait life isnt box thats going Stone enjoy s...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>groove annoying Beyond consuming watch liquor bad soundtrack redeemed girl Springtime spark turkey thousan...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>story dont loss Diane religion turn toSo emotional Sister mom tragedy Judaism yesterday Jesuit bad practic...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>story film give plenty pin reaction modern bring part Charlton aside underdeveloped typically vicious put ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>including best McCoy fan implausible expects watch known far art interact however see well cant cable youl...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                              review  \\\n",
       "0      right Security hardcore unflinching GO guard City Christians glass watched may called timid wordIt thing b...   \n",
       "1      give sense guard discomforting really guided Orton master surface thing dream extremely got unassuming lif...   \n",
       "2      likable Match right Risk theater hot Addiction Prada Woody interesting Wears womanThis air spend may disap...   \n",
       "3      film make Decide thriller BOOGEYMAN soap youre timeThis 10 boy zombie zombieOK watched ruin real life shot...   \n",
       "4      loneliness best film make place look Imperioli Carol meet watch Money find Arthur connected aliveWe thing ...   \n",
       "...                                                                                                              ...   \n",
       "49995  right job always pay DVD watch touching original people count wait life isnt box thats going Stone enjoy s...   \n",
       "49996  groove annoying Beyond consuming watch liquor bad soundtrack redeemed girl Springtime spark turkey thousan...   \n",
       "49997  story dont loss Diane religion turn toSo emotional Sister mom tragedy Judaism yesterday Jesuit bad practic...   \n",
       "49998  story film give plenty pin reaction modern bring part Charlton aside underdeveloped typically vicious put ...   \n",
       "49999  including best McCoy fan implausible expects watch known far art interact however see well cant cable youl...   \n",
       "\n",
       "      sentiment  \n",
       "0      positive  \n",
       "1      positive  \n",
       "2      positive  \n",
       "3      negative  \n",
       "4      positive  \n",
       "...         ...  \n",
       "49995  positive  \n",
       "49996  negative  \n",
       "49997  negative  \n",
       "49998  negative  \n",
       "49999  negative  \n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set a variable for the normalized dataframe and add the data_IMDB\n",
    "norm_data_IMDB = data_IMDB.copy()\n",
    "norm_data_IMDB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 292,
     "status": "ok",
     "timestamp": 1699964095794,
     "user": {
      "displayName": "Parth Uday",
      "userId": "17936957073811628306"
     },
     "user_tz": -480
    },
    "id": "72xBxKCGi267",
    "outputId": "90504563-38d6-4432-ee3a-80879f4ab236"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_data_IMDB.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MomAYMLdleo-"
   },
   "source": [
    "## 3. Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XLy9vhfxg4kf"
   },
   "source": [
    "### Method 1: Bag of Words\n",
    "\n",
    "The \"Bag of Words\" (BoW) model is a common and simple representation used in natural language processing (NLP) and information retrieval.\n",
    "\n",
    "It's a way of converting text data into numerical vectors that can be used by machine learning algorithms\n",
    "\n",
    "TLDR: Based on the raw word counts and is suitable when you want to capture the frequency of words in a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 417,
     "status": "ok",
     "timestamp": 1699964220494,
     "user": {
      "displayName": "Parth Uday",
      "userId": "17936957073811628306"
     },
     "user_tz": -480
    },
    "id": "mmNYt8w9P5io",
    "outputId": "b73f9c6d-72bc-4204-dc4c-8c9915e28c34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   and  and this  and this is  document  document is  document is the  first  \\\n",
      "0    0         0            0         1            0                0      1   \n",
      "1    0         0            0         2            1                1      0   \n",
      "2    1         1            1         0            0                0      0   \n",
      "3    0         0            0         1            0                0      1   \n",
      "\n",
      "   first document  is  is the  ...  the third one  third  third one  this  \\\n",
      "0               1   1       1  ...              0      0          0     1   \n",
      "1               0   1       1  ...              0      0          0     1   \n",
      "2               0   1       1  ...              1      1          1     1   \n",
      "3               1   1       0  ...              0      0          0     1   \n",
      "\n",
      "   this document  this document is  this is  this is the  this the  \\\n",
      "0              0                 0        1            1         0   \n",
      "1              1                 1        0            0         0   \n",
      "2              0                 0        1            1         0   \n",
      "3              0                 0        0            0         1   \n",
      "\n",
      "   this the first  \n",
      "0               0  \n",
      "1               0  \n",
      "2               0  \n",
      "3               1  \n",
      "\n",
      "[4 rows x 34 columns]\n"
     ]
    }
   ],
   "source": [
    "# Example documents in list form\n",
    "documents = [\"This is the first document.\",\n",
    "              \"This document is the second document.\",\n",
    "              \"And this is the third one.\",\n",
    "              \"Is this the first document?\"]\n",
    "\n",
    "# Create an instance of the CountVectorizer class, where ngram ranges from 1 word to 3 words\n",
    "# Unigram = singular word / Bigram = 2 words\n",
    "vectorizer = CountVectorizer(ngram_range=(1,3))\n",
    "\n",
    "# Fit and transform the documents into a Bag of Words representation\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Get the feature names (words) that correspond to the columns in the Bag of Words matrix\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert the Bag of Words matrix to an array for better visualization\n",
    "X_array = X.toarray()\n",
    "\n",
    "# DataFrame for better visualization\n",
    "df_bow = pd.DataFrame(X_array, columns = feature_names)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "43qMqyzyvPN6"
   },
   "outputs": [],
   "source": [
    "# Fitting our data into the CountVectorizer\n",
    "vect = CountVectorizer(ngram_range=(1,3)).fit(norm_data_IMDB['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "id": "eY7GWKlPwZDT"
   },
   "outputs": [],
   "source": [
    "# Getting the feature names from the vectorised features\n",
    "feature_names = vect.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 342,
     "status": "ok",
     "timestamp": 1699964447722,
     "user": {
      "displayName": "Parth Uday",
      "userId": "17936957073811628306"
     },
     "user_tz": -480
    },
    "id": "92o5NyXJUfl8",
    "outputId": "d954069d-8bb9-4adf-f0dd-afc06ea2e7a1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['00', '00 exclusively', '00 exclusively camera', ...,\n",
       "       'zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz',\n",
       "       'zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz stiff',\n",
       "       'zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz stiff projected'], dtype=object)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 342,
     "status": "ok",
     "timestamp": 1699964471392,
     "user": {
      "displayName": "Parth Uday",
      "userId": "17936957073811628306"
     },
     "user_tz": -480
    },
    "id": "rdbMi9f7acuY",
    "outputId": "931958b9-acdb-4d07-a739-ed1bac5ded8e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    right Security hardcore unflinching GO guard City Christians glass watched may called timid wordIt thing b...\n",
       "1    give sense guard discomforting really guided Orton master surface thing dream extremely got unassuming lif...\n",
       "2    likable Match right Risk theater hot Addiction Prada Woody interesting Wears womanThis air spend may disap...\n",
       "3    film make Decide thriller BOOGEYMAN soap youre timeThis 10 boy zombie zombieOK watched ruin real life shot...\n",
       "4    loneliness best film make place look Imperioli Carol meet watch Money find Arthur connected aliveWe thing ...\n",
       "Name: review, dtype: object"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_data_IMDB['review'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "id": "2ixbcLk4wtIh"
   },
   "outputs": [],
   "source": [
    "# Extract the feature 'review'\n",
    "X_cv = norm_data_IMDB['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 321,
     "status": "ok",
     "timestamp": 1699964516939,
     "user": {
      "displayName": "Parth Uday",
      "userId": "17936957073811628306"
     },
     "user_tz": -480
    },
    "id": "Ilqr94ZyRgKQ",
    "outputId": "53fc2030-6c03-4007-bced-0c4aeab18732"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000,)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_cv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "id": "AJQgcjzuwzfL"
   },
   "outputs": [],
   "source": [
    "# Extract the target 'sentiment'\n",
    "Y_cv = norm_data_IMDB['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "id": "JXtC_bJFw-nr"
   },
   "outputs": [],
   "source": [
    "# Transforming the feature 'review' data\n",
    "X_cv = vect.transform(X_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1699964601143,
     "user": {
      "displayName": "Parth Uday",
      "userId": "17936957073811628306"
     },
     "user_tz": -480
    },
    "id": "mITBHBOHZw12",
    "outputId": "10369e45-84ae-40cc-aadd-a443136346a5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 6445135)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_cv.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RFM3ueoMk4Qw"
   },
   "source": [
    "#### Method 2: TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JGQVk1RwDwdG"
   },
   "source": [
    "Term Frequency (TF):\n",
    "\n",
    "The TF component measures how often a term appears in a document. It's a raw count of the number of times the term occurs within the document.\n",
    "TF is calculated for each term within each document.\n",
    "\n",
    "Inverse Document Frequency (IDF):\n",
    "\n",
    "The IDF component evaluates how important a term is across the entire corpus(enitre body of text). It's a measure of how unique or rare a term is.\n",
    "Terms that appear frequently in many documents have a lower IDF, while terms that appear in a smaller subset of documents have a higher IDF.\n",
    "\n",
    "TLDR: Considers not only the frequency of words but also their importance across the entire set of documents. It helps in emphasizing words that are more discriminative and less common across documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "id": "by0RVet1xy29"
   },
   "outputs": [],
   "source": [
    "# Create TFIDF vectorizer\n",
    "tfidf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "id": "RlpSc6cDx2tO"
   },
   "outputs": [],
   "source": [
    "# Apply TFIDF transformer to 'review' column\n",
    "X_tf = tfidf.fit_transform(norm_data_IMDB['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 308,
     "status": "ok",
     "timestamp": 1699964747792,
     "user": {
      "displayName": "Parth Uday",
      "userId": "17936957073811628306"
     },
     "user_tz": -480
    },
    "id": "yUjgr06JyDlh",
    "outputId": "66ba1ef7-c99c-46ea-86fa-1184ee5dc7df"
   },
   "outputs": [],
   "source": [
    "# tfidf.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 377,
     "status": "ok",
     "timestamp": 1699964764120,
     "user": {
      "displayName": "Parth Uday",
      "userId": "17936957073811628306"
     },
     "user_tz": -480
    },
    "id": "_e7PzJczyNES",
    "outputId": "baa3f39c-452b-4ba8-e9e5-409c59b8be61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 215976)\n"
     ]
    }
   ],
   "source": [
    "print(X_tf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "id": "cMYTtn3yypB-"
   },
   "outputs": [],
   "source": [
    "# Extract the target 'sentiment'\n",
    "Y_tf = norm_data_IMDB['sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kkYW8Q6Ak_tf"
   },
   "source": [
    "### Labelling the 'sentiment' text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 393,
     "status": "ok",
     "timestamp": 1699964891484,
     "user": {
      "displayName": "Parth Uday",
      "userId": "17936957073811628306"
     },
     "user_tz": -480
    },
    "id": "gF-yWm6nlCOf",
    "outputId": "8c211f08-4e87-44df-ca48-87daf2adda3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 1)\n"
     ]
    }
   ],
   "source": [
    "# Setting up the LabelBinarizer\n",
    "lb = LabelBinarizer()\n",
    "\n",
    "# Transforming and Labelling the 'sentiment' data\n",
    "sentiment_data = lb.fit_transform(data_IMDB['sentiment'])\n",
    "print(sentiment_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1],\n",
       "       ...,\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GGGWtvOMliTf"
   },
   "source": [
    "## 4. ML Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TOhdJJDZ9cJc"
   },
   "source": [
    "### Model 1: Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3MJbEOPoVm5U"
   },
   "source": [
    "#### Logistic Regression - Bags of Words Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "sD5S3amNzHRa"
   },
   "outputs": [],
   "source": [
    "# Setting up the LogisticRegression model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "Au45psF3D2Ja"
   },
   "outputs": [],
   "source": [
    "# # Split arrays/matrices into random train and test subsets. In this case, 80:20 for Train:Test ratio\n",
    "# x_train_cv, x_test_cv, y_train_cv, y_test_cv = train_test_split(X_cv, Y_cv, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 198736,
     "status": "ok",
     "timestamp": 1699965129056,
     "user": {
      "displayName": "Parth Uday",
      "userId": "17936957073811628306"
     },
     "user_tz": -480
    },
    "id": "0moO6dI1VlCk",
    "outputId": "c1accca3-14c0-406a-dbd2-12efe5a5fa74"
   },
   "outputs": [],
   "source": [
    "# Fitting the lr model for Bag of Words\n",
    "\n",
    "\n",
    "\n",
    "# Predicting the lr model for Bag of Words\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GZf1teCgVxJM"
   },
   "source": [
    "#### Logistic Regression - TFIDF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "HVfCailSD7fX"
   },
   "outputs": [],
   "source": [
    "# # Split arrays/matrices into random train and test subsets. In this case, 80:20 for Train:Test ratio\n",
    "# x_train_tf, x_test_tf, y_train_tf, y_test_tf = train_test_split(X_tf, Y_tf, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15502,
     "status": "ok",
     "timestamp": 1699965328302,
     "user": {
      "displayName": "Parth Uday",
      "userId": "17936957073811628306"
     },
     "user_tz": -480
    },
    "id": "O8Et6na74Dq6",
    "outputId": "bfb958cc-8e50-46c0-fed3-1333507f89b3"
   },
   "outputs": [],
   "source": [
    "# # Fitting the lr model for TFIDF features\n",
    "# lr_tfidf = lr.fit(x_train_tf, y_train_tf)\n",
    "# print(lr_tfidf)\n",
    "\n",
    "# # Predicting the lr model for TFIDF features\n",
    "# lr_tfidf_predict = lr.predict(x_test_tf)\n",
    "# print(lr_tfidf_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I92eyxS364-O"
   },
   "source": [
    "#### Logistic Regression - Accuracy Scores & Classification Report for both Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 533,
     "status": "ok",
     "timestamp": 1699965332552,
     "user": {
      "displayName": "Parth Uday",
      "userId": "17936957073811628306"
     },
     "user_tz": -480
    },
    "id": "VWI9pVdM6zg6",
    "outputId": "4ad8a685-5aed-433e-e82c-62ebcd4c407f"
   },
   "outputs": [],
   "source": [
    "# Accuracy score for Bag of Words\n",
    "\n",
    "\n",
    "\n",
    "# Accuracy score for TFIDF features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 491,
     "status": "ok",
     "timestamp": 1699934183667,
     "user": {
      "displayName": "Parth Uday",
      "userId": "17936957073811628306"
     },
     "user_tz": -480
    },
    "id": "LqVeOEMU7o8B",
    "outputId": "3d6415cc-7403-4d95-c756-3c4519abf093"
   },
   "outputs": [],
   "source": [
    "# Classification report for Bag of Words\n",
    "\n",
    "\n",
    "\n",
    "# Classification report for TFIDF features\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vKvSqPOi8RN-"
   },
   "source": [
    "#### Logistic Regression - Confusion Matrix for both Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KcBlCbcgWubg"
   },
   "source": [
    "##### For Bags of Words Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 481
    },
    "executionInfo": {
     "elapsed": 1571,
     "status": "ok",
     "timestamp": 1699965658821,
     "user": {
      "displayName": "Parth Uday",
      "userId": "17936957073811628306"
     },
     "user_tz": -480
    },
    "id": "CHnsIfNE8N6O",
    "outputId": "6ab01464-012e-4635-9b8b-61d07ba3c878"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vqTratyrWxVG"
   },
   "source": [
    "##### For TFIDF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "executionInfo": {
     "elapsed": 941,
     "status": "ok",
     "timestamp": 1699965786848,
     "user": {
      "displayName": "Parth Uday",
      "userId": "17936957073811628306"
     },
     "user_tz": -480
    },
    "id": "AYKRQGjl8atp",
    "outputId": "2cdd78bf-abd3-4011-c3fa-ba2ca868fa9e"
   },
   "outputs": [],
   "source": [
    "# cm_tf = confusion_matrix(y_test_tf, lr_tfidf_predict, labels=lr.classes_)\n",
    "# disp = ConfusionMatrixDisplay(confusion_matrix=cm_tf, display_labels=lr.classes_)\n",
    "# disp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tAELwplY9lRO"
   },
   "source": [
    "### Model 2: Multinomial Naive Bayes (MNB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2qBO_5xuWFOn"
   },
   "source": [
    "#### MNB - Bags of Words Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1747,
     "status": "ok",
     "timestamp": 1699965969576,
     "user": {
      "displayName": "Parth Uday",
      "userId": "17936957073811628306"
     },
     "user_tz": -480
    },
    "id": "q5wkjSj99sDu",
    "outputId": "eb13a93f-5fb8-4a99-d935-1dea52e8a057"
   },
   "outputs": [],
   "source": [
    "# Training the Multinomial Naive Bayes model\n",
    "\n",
    "\n",
    "\n",
    "# Fitting the MNB for Bag of Words\n",
    "\n",
    "\n",
    "\n",
    "# Predicting the model for Bag of Words\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HsZAdvhoWKr3"
   },
   "source": [
    "#### MNB - TFIDF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 669,
     "status": "ok",
     "timestamp": 1699965883012,
     "user": {
      "displayName": "Parth Uday",
      "userId": "17936957073811628306"
     },
     "user_tz": -480
    },
    "id": "ovMwFrp991vq",
    "outputId": "5d54ebb6-e594-446b-c65d-519c1c8591f5"
   },
   "outputs": [],
   "source": [
    "# # Fitting the MNB for TFIDF features\n",
    "# mnb_tfidf = mnb.fit(x_train_tf, y_train_tf)\n",
    "# print(mnb_tfidf)\n",
    "\n",
    "# # Predicting the MNB model for TFIDF features\n",
    "# mnb_tfidf_predict = mnb.predict(x_test_tf)\n",
    "# print(mnb_tfidf_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-hj3qyuKWOoK"
   },
   "source": [
    "#### MNB - Accuracy Scores for both Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 312,
     "status": "ok",
     "timestamp": 1699966061174,
     "user": {
      "displayName": "Parth Uday",
      "userId": "17936957073811628306"
     },
     "user_tz": -480
    },
    "id": "FxVvLhgd9_Z5",
    "outputId": "5f02624d-8c65-49ac-d8bc-649fbf158444"
   },
   "outputs": [],
   "source": [
    "# # Accuracy score for Bag of Words\n",
    "# mnb_bow_score = accuracy_score(y_test_cv, mnb_bow_predict)\n",
    "# print('mnb_bow_score : {:.2f}%'.format(mnb_bow_score*100))\n",
    "\n",
    "# # Accuracy score for TFIDF features\n",
    "# mnb_tfidf_score = accuracy_score(y_test_tf, mnb_tfidf_predict)\n",
    "# print('mnb_tfidf_score : {:.2f}%'.format(mnb_tfidf_score*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XDWkyGJuWSOE"
   },
   "source": [
    "#### MNB - Confusion Matrix for both Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-KQkYOrOWYjT"
   },
   "source": [
    "##### For Bags of Words Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "executionInfo": {
     "elapsed": 1641,
     "status": "ok",
     "timestamp": 1699966370338,
     "user": {
      "displayName": "Parth Uday",
      "userId": "17936957073811628306"
     },
     "user_tz": -480
    },
    "id": "fRqQVgDC-mX5",
    "outputId": "e606d671-f3d0-4a01-f2b0-856300204427"
   },
   "outputs": [],
   "source": [
    "# cm_cv_mnb = confusion_matrix(y_test_cv, mnb_bow_predict, labels=mnb.classes_)\n",
    "# disp = ConfusionMatrixDisplay(confusion_matrix = cm_cv_mnb, display_labels = mnb.classes_)\n",
    "# disp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PastJOQ7WfV1"
   },
   "source": [
    "##### For TFIDF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "executionInfo": {
     "elapsed": 1672,
     "status": "ok",
     "timestamp": 1699966388056,
     "user": {
      "displayName": "Parth Uday",
      "userId": "17936957073811628306"
     },
     "user_tz": -480
    },
    "id": "9M0Yo9Vm_AXk",
    "outputId": "3647239a-87e7-4814-a80f-8e6217cb4b30"
   },
   "outputs": [],
   "source": [
    "# cm_tf_mnb = confusion_matrix(y_test_tf, mnb_tfidf_predict, labels=mnb.classes_)\n",
    "# disp = ConfusionMatrixDisplay(confusion_matrix = cm_tf_mnb, display_labels = mnb.classes_)\n",
    "# disp.plot()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "XLy9vhfxg4kf",
    "kkYW8Q6Ak_tf",
    "I92eyxS364-O",
    "KcBlCbcgWubg",
    "vqTratyrWxVG"
   ],
   "provenance": [
    {
     "file_id": "1zcrp3gK6repODBPmrSq7T1X1HmQKKlvm",
     "timestamp": 1700239150862
    },
    {
     "file_id": "1KXITORjL7oLNg0MqRKuMoLm3iN7UfrcL",
     "timestamp": 1699955804539
    }
   ]
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
